{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "baseline_large_minibatch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn9jD0ddjqmT"
      },
      "source": [
        "# Optimization for machine learning - Mini Project: Large mini-batch baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j9KGi5Lkrv2"
      },
      "source": [
        "## 1. Loading libraries, modules and setting directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1aMbByLlBoE"
      },
      "source": [
        "## Run this cell if PyTorch is not installed on Colab\n",
        "#!pip3 install torch torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcZo9dv01Rip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9662f3d-1ba8-4750-e457-bb5761160ccf"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount= True)\n",
        "\n",
        "import sys\n",
        "sys.path.append('gdrive/My Drive/Colab Notebooks/OptML')\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from models import CNN as CNN\n",
        "\n",
        "import time\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax31bmofl3S1"
      },
      "source": [
        "### Creating device and checking GPU availability "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2LMdii1k-2X",
        "outputId": "c3e38cd6-c018-440e-a1bd-5abc4c65a9ef"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNyqYzFc1Rir"
      },
      "source": [
        "## 2. Downloading and normalizing CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NmaInPEl7a6"
      },
      "source": [
        "# Normalize dataset and transform into tensor\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaFzwFTyl-XH",
        "outputId": "6e3d5b03-a9cd-471a-991d-684b81908423"
      },
      "source": [
        "# Load dataset and convert images to tensors\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "used_categories = range(len(classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT_H1wu01Riv"
      },
      "source": [
        "## 3. Training the network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "626-FeK4pnta"
      },
      "source": [
        "#### Training and testing is set up with a simple validation loop (no CV), looping over for the trainset for a specified number of epochs. At every epoch, the loss, accuracy and time is captured as raw data for later plotting. If GPU was available the model is transferred to the device along with a transformation of the tensor into cuda. To load the data for every batchsize, the torch Dataloader has been utilized. The Cross Entropy loss is used as the loss function and the optimizer used througout the training is the SGD (no momentum or weight-decay). Learning rate equal to 0.001 for all of the small minibatches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQxeNqqx1Riv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b92316-7afb-4097-9d3b-af46ccca4ab0"
      },
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Batch sizes and number of epochs - as there is no need to run 400 epochs for the smaller batch sizes (128, 256), we suggest the following setup for running code below:\n",
        "# (batches, num_epochs): (128, 100), (256, 150), (512, 200), (1048, 300), (2048, 400)\n",
        "batches = [128, 256, 512, 1024, 2048]\n",
        "num_epoch = 400\n",
        "\n",
        "# Lists for storing results\n",
        "accuracies = []\n",
        "epochs = []\n",
        "time_pr_epoch = []\n",
        "batch_sizes = []\n",
        "loss_pr_epoch = []\n",
        "\n",
        "for BATCH_SIZE in batches:\n",
        "  model = CNN()\n",
        "  model.to(device)\n",
        "  print(\"######### BATCH SIZE = \",BATCH_SIZE,\" ######### \" )\n",
        "\n",
        "  # Initialize trainloader and test loader   \n",
        "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "  criterion =  torch.nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
        "  for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "      start_time = time.time()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_loss_for_epoch = 0.0\n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "          \n",
        "\n",
        "          # wrap them in Variable\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs,labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          running_loss_for_epoch += loss.item()\n",
        "          if i % 100 == 99:    # print every 100 mini-batches\n",
        "              print('[epoch : %d, minibatch : %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 100))\n",
        "              running_loss = 0.0\n",
        "\n",
        "          correct = 0\n",
        "          total = 0\n",
        "\n",
        "      for i, data in enumerate(testloader, 0):\n",
        "          inputs, labels = data\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          #outputs = model(Variable(inputs))\n",
        "          outputs = model(inputs)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "      print('Accuracy of the network on the {} test images: {:4.2f} %'.format(\n",
        "          testset.data.shape[0], 100 * (correct / total)))\n",
        "\n",
        "      finish_time = (time.time() - start_time)\n",
        "      batches_pr_epoch = math.ceil(trainset.data.shape[0] / BATCH_SIZE)\n",
        "      loss_pr_epoch.append(running_loss_for_epoch / batches_pr_epoch)\n",
        "      accuracies.append(100 * (correct / total))\n",
        "      batch_sizes.append(BATCH_SIZE)\n",
        "      epochs.append(epoch)\n",
        "      time_pr_epoch.append(finish_time)\n",
        "      \n",
        "    \n",
        "  print('Finished Training')\n",
        "\n",
        "  \n",
        "print(\"loss pr epoch\", loss_pr_epoch)\n",
        "print(\"accuracies\", accuracies)\n",
        "print(\"batch_sizes\", batch_sizes)\n",
        "print(\"epochs\", epochs)\n",
        "print(\"time_pr_epoch\", time_pr_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######### BATCH SIZE =  16  ######### \n",
            "[epoch : 1, minibatch :   100] loss: 2.302\n",
            "[epoch : 1, minibatch :   200] loss: 2.300\n",
            "[epoch : 1, minibatch :   300] loss: 2.294\n",
            "[epoch : 1, minibatch :   400] loss: 2.292\n",
            "[epoch : 1, minibatch :   500] loss: 2.289\n",
            "[epoch : 1, minibatch :   600] loss: 2.283\n",
            "[epoch : 1, minibatch :   700] loss: 2.277\n",
            "[epoch : 1, minibatch :   800] loss: 2.268\n",
            "[epoch : 1, minibatch :   900] loss: 2.262\n",
            "[epoch : 1, minibatch :  1000] loss: 2.256\n",
            "[epoch : 1, minibatch :  1100] loss: 2.242\n",
            "[epoch : 1, minibatch :  1200] loss: 2.230\n",
            "[epoch : 1, minibatch :  1300] loss: 2.215\n",
            "[epoch : 1, minibatch :  1400] loss: 2.191\n",
            "[epoch : 1, minibatch :  1500] loss: 2.165\n",
            "[epoch : 1, minibatch :  1600] loss: 2.128\n",
            "[epoch : 1, minibatch :  1700] loss: 2.091\n",
            "[epoch : 1, minibatch :  1800] loss: 2.051\n",
            "[epoch : 1, minibatch :  1900] loss: 2.021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umz_O_Z9t0Pj"
      },
      "source": [
        "## 4. Saving results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHPU-2gbuAE6"
      },
      "source": [
        "#### Results are saved in a tab-seperated csv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p87SYVOAjH6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = pd.DataFrame(\n",
        "    {'batch_size': batch_sizes,\n",
        "     'epoch': epochs,\n",
        "     'accuracy': accuracies,\n",
        "     'loss': loss_pr_epoch,\n",
        "     'time': time_pr_epoch\n",
        "    })\n",
        "\n",
        "results.to_csv('gdrive/MyDrive/results_large_batches_wlr_2048.csv', sep ='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
